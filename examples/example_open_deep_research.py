"""
ä½¿ç”¨ create_deep_agent_customized å®ç° Open Deep Research Agent

è¿™ä¸ªå®ç°åŸºäº LangGraph çš„ open_deep_research é¡¹ç›®ï¼Œä½¿ç”¨ deepagents æ¡†æ¶é‡æ–°å®ç°ã€‚

æ ¸å¿ƒæ¶æ„ï¼š
1. ä¸»ä»£ç† (Supervisor): åˆ†æç ”ç©¶é—®é¢˜ï¼Œåˆ¶å®šç ”ç©¶è®¡åˆ’ï¼Œå§”æ´¾å­ä»£ç†æ‰§è¡Œç ”ç©¶
2. ç ”ç©¶å­ä»£ç† (Researcher): æ‰§è¡Œå…·ä½“çš„ç½‘ç»œæœç´¢å’Œä¿¡æ¯æ”¶é›†
3. æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆï¼šæ±‡æ€»æ‰€æœ‰ç ”ç©¶ç»“æœï¼Œç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š

ç‰¹ç‚¹ï¼š
- æ”¯æŒå¤šè½®å¯¹è¯å’Œæ¾„æ¸…é—®é¢˜
- æ”¯æŒå¹¶è¡Œç ”ç©¶å¤šä¸ªå­ä¸»é¢˜
- è‡ªåŠ¨å‹ç¼©å’Œç®¡ç†ä¸Šä¸‹æ–‡
- ç”Ÿæˆå¸¦å¼•ç”¨çš„ä¸“ä¸šç ”ç©¶æŠ¥å‘Š
"""

import os
import uuid
import asyncio
from datetime import datetime
from typing import Literal
import requests

from dotenv import load_dotenv
from tavily import TavilyClient, AsyncTavilyClient
from langchain.chat_models import init_chat_model
from langchain_core.tools import tool
from langgraph.checkpoint.memory import MemorySaver
from deepagents.backends import FilesystemBackend

from agent_templates import create_deep_agent_customized

load_dotenv()

# ============ é…ç½® ============
# API é…ç½®
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "deepseek-chat")

# LLM å‚æ•°
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.7"))
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "8192"))

# Agent é…ç½®
DEFAULT_CONTEXT_LIMIT = int(os.getenv("DEFAULT_CONTEXT_LIMIT", "128000"))

# Tavily é…ç½®
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

# åšæŸ¥é…ç½®
USE_BOCHAI_SEARCH = os.getenv("USE_BOCHAI_SEARCH", "false").lower() == "true"
BOCHAI_API_KEY = os.getenv("BOCHAI_API_KEY")
BOCHAI_BASE_URL = os.getenv("BOCHAI_BASE_URL", "https://api.bocha.cn/v1/web-search")

# ç ”ç©¶é…ç½®
MAX_SEARCH_RESULTS = int(os.getenv("MAX_SEARCH_RESULTS", "5"))
MAX_CONCURRENT_RESEARCH = int(os.getenv("MAX_CONCURRENT_RESEARCH", "3"))

# å·¥ä½œç›®å½•
WORKSPACE_DIR = os.path.join(os.path.dirname(__file__), "temp")
os.makedirs(WORKSPACE_DIR, exist_ok=True)


# ============ è¾…åŠ©å‡½æ•° ============
def get_today_str() -> str:
    """è·å–ä»Šå¤©çš„æ—¥æœŸå­—ç¬¦ä¸²"""
    return datetime.now().strftime("%Y-%m-%d")


# ============ Prompts ============
CLARIFY_WITH_USER_PROMPT = """åˆ†æç”¨æˆ·çš„ç ”ç©¶è¯·æ±‚ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ¾„æ¸…é—®é¢˜ã€‚

ç”¨æˆ·æ¶ˆæ¯:
{messages}

ä»Šå¤©æ—¥æœŸ: {date}

è¯„ä¼°æ ‡å‡†:
1. ç ”ç©¶ä¸»é¢˜æ˜¯å¦æ¸…æ™°æ˜ç¡®ï¼Ÿ
2. æ˜¯å¦å­˜åœ¨æ¨¡ç³Šçš„ç¼©å†™æˆ–ä¸“ä¸šæœ¯è¯­ï¼Ÿ
3. ç ”ç©¶èŒƒå›´æ˜¯å¦åˆç†ï¼Ÿ

å¦‚æœéœ€è¦æ¾„æ¸…:
- æå‡ºç®€æ´æ˜ç¡®çš„é—®é¢˜
- ä½¿ç”¨é¡¹ç›®ç¬¦å·åˆ—å‡ºéœ€è¦æ¾„æ¸…çš„ç‚¹

å¦‚æœä¸éœ€è¦æ¾„æ¸…:
- ç¡®è®¤ç†è§£ç”¨æˆ·çš„ç ”ç©¶éœ€æ±‚
- ç®€è¦æ€»ç»“ç ”ç©¶è¦ç‚¹
- è¡¨ç¤ºå³å°†å¼€å§‹ç ”ç©¶
"""

RESEARCH_BRIEF_PROMPT = """å°†ç”¨æˆ·çš„ç ”ç©¶è¯·æ±‚è½¬åŒ–ä¸ºè¯¦ç»†çš„ç ”ç©¶ç®€æŠ¥ã€‚

ç”¨æˆ·æ¶ˆæ¯:
{messages}

ä»Šå¤©æ—¥æœŸ: {date}

è¦æ±‚:
1. æœ€å¤§åŒ–å…·ä½“æ€§å’Œç»†èŠ‚
2. åŒ…å«ç”¨æˆ·æåˆ°çš„æ‰€æœ‰åå¥½å’Œè¦æ±‚
3. å¯¹æœªæŒ‡å®šçš„å¿…è¦ç»´åº¦ä¿æŒå¼€æ”¾
4. ä½¿ç”¨ç¬¬ä¸€äººç§°
5. å¦‚æœæœ‰ç‰¹å®šæ¥æºåå¥½ï¼Œè¯·è¯´æ˜
"""

SUPERVISOR_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªç ”ç©¶ä¸»ç®¡ã€‚ä½ çš„å·¥ä½œæ˜¯é€šè¿‡è°ƒç”¨å­ä»£ç†æ¥è¿›è¡Œæ·±åº¦ç ”ç©¶ã€‚ä»Šå¤©æ—¥æœŸ: {date}

## ä»»åŠ¡
ä½ éœ€è¦åˆ†æç”¨æˆ·çš„ç ”ç©¶é—®é¢˜ï¼Œå°†å…¶åˆ†è§£ä¸ºå¯ç®¡ç†çš„å­ä»»åŠ¡ï¼Œç„¶åå§”æ´¾ç»™ä¸“é—¨çš„ç ”ç©¶å­ä»£ç†æ‰§è¡Œã€‚

## å¯ç”¨å·¥å…·
1. **task** - è°ƒç”¨ç ”ç©¶å­ä»£ç†æ‰§è¡Œå…·ä½“æœç´¢ä»»åŠ¡ã€‚æ ¼å¼: task("researcher", "å…·ä½“ç ”ç©¶ä¸»é¢˜")
2. **write_todos** - åˆ¶å®šç ”ç©¶è®¡åˆ’
3. **think** - åæ€å’Œè§„åˆ’ï¼ˆæ¯æ¬¡è°ƒç”¨å­ä»£ç†å‰åéƒ½åº”ä½¿ç”¨ï¼‰
4. **write_file** - ä¿å­˜ç ”ç©¶æŠ¥å‘Šåˆ°æ–‡ä»¶ã€‚æ ¼å¼: write_file("/æŠ¥å‘Šå.md", æŠ¥å‘Šå†…å®¹)
5. **ls** - åˆ—å‡ºç›®å½•å†…å®¹
6. **read_file** - è¯»å–æ–‡ä»¶å†…å®¹

## æ–‡ä»¶ç³»ç»Ÿ
- å½“å‰å·¥ä½œç›®å½•æ˜¯æ ¹ç›®å½• `/`
- ç ”ç©¶æŠ¥å‘Šå¿…é¡»ä¿å­˜åˆ°æ–‡ä»¶ï¼Œä½¿ç”¨ `write_file("/ç ”ç©¶æŠ¥å‘Š_ä¸»é¢˜.md", å†…å®¹)` ä¿å­˜
- æ–‡ä»¶ååº”è¯¥ç®€æ´æ˜äº†ï¼ŒåŒ…å«ç ”ç©¶ä¸»é¢˜

## å·¥ä½œæµç¨‹

### ç¬¬ä¸€æ­¥ï¼šåˆ†æä¸è§„åˆ’
æ”¶åˆ°ç ”ç©¶é—®é¢˜åï¼Œé¦–å…ˆä½¿ç”¨ think å·¥å…·åˆ†æï¼š
- è¿™ä¸ªé—®é¢˜éœ€è¦å“ªäº›æ–¹é¢çš„ä¿¡æ¯ï¼Ÿ
- æ˜¯å¦å¯ä»¥åˆ†è§£ä¸ºç‹¬ç«‹çš„å­ç ”ç©¶ï¼Ÿ
- éœ€è¦å¤šå°‘ä¸ªå¹¶è¡Œç ”ç©¶ï¼Ÿ

### ç¬¬äºŒæ­¥ï¼šå§”æ´¾ç ”ç©¶
- å¯¹äºç®€å•æŸ¥è¯¢ï¼šä½¿ç”¨ 1 ä¸ªå­ä»£ç†
- å¯¹äºæ¯”è¾ƒç±»æŸ¥è¯¢ï¼šä¸ºæ¯ä¸ªæ¯”è¾ƒå¯¹è±¡åˆ†é…å­ä»£ç†
- å¯¹äºå¤æ‚æŸ¥è¯¢ï¼šå°†é—®é¢˜åˆ†è§£ä¸º 2-{max_concurrent} ä¸ªç‹¬ç«‹å­ä»»åŠ¡

### ç¬¬ä¸‰æ­¥ï¼šè¯„ä¼°ç»“æœ
æ¯æ¬¡å­ä»£ç†è¿”å›åï¼Œä½¿ç”¨ think å·¥å…·è¯„ä¼°ï¼š
- è·å¾—äº†å“ªäº›å…³é”®ä¿¡æ¯ï¼Ÿ
- è¿˜ç¼ºå°‘ä»€ä¹ˆï¼Ÿ
- æ˜¯å¦è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Ÿ

### ç¬¬å››æ­¥ï¼šç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
å½“æ”¶é›†åˆ°è¶³å¤Ÿä¿¡æ¯åï¼š
1. æ’°å†™æœ€ç»ˆç ”ç©¶æŠ¥å‘Šï¼š
   - ä½¿ç”¨æ¸…æ™°çš„ç»“æ„ï¼ˆæ ‡é¢˜ã€ç« èŠ‚ã€å°èŠ‚ï¼‰
   - åŒ…å«æ‰€æœ‰ç›¸å…³å‘ç°å’Œå¼•ç”¨
   - ä½¿ç”¨ [æ ‡é¢˜](URL) æ ¼å¼å¼•ç”¨æ¥æº
   - åœ¨æœ«å°¾åˆ—å‡ºæ‰€æœ‰æ¥æº
2. **å¿…é¡»ä½¿ç”¨ write_file å·¥å…·å°†æŠ¥å‘Šä¿å­˜åˆ°æ–‡ä»¶**ï¼š
   - æ ¼å¼ï¼š`write_file("/ç ”ç©¶æŠ¥å‘Š_ä¸»é¢˜åç§°.md", æŠ¥å‘Šå†…å®¹)`
   - ä¾‹å¦‚ï¼š`write_file("/ç ”ç©¶æŠ¥å‘Š_AIæ¨¡å‹å¯¹æ¯”.md", "# AIæ¨¡å‹å¯¹æ¯”...")`
3. å‘ç”¨æˆ·ç¡®è®¤æŠ¥å‘Šå·²ä¿å­˜ï¼Œå¹¶ç»™å‡ºç®€è¦æ€»ç»“

## é™åˆ¶
- æœ€å¤šå¹¶è¡Œ {max_concurrent} ä¸ªç ”ç©¶ä»»åŠ¡
- æœ€å¤šè¿›è¡Œ {max_iterations} è½®ç ”ç©¶è¿­ä»£
- å½“æœ‰è¶³å¤Ÿä¿¡æ¯å›ç­”é—®é¢˜æ—¶ç«‹å³åœæ­¢
- **å®Œæˆç ”ç©¶åå¿…é¡»ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶**

## è¾“å‡ºæ ¼å¼
æœ€ç»ˆæŠ¥å‘Šåº”è¯¥åŒ…å«ï¼š
1. æ¦‚è¿°/å¼•è¨€
2. ä¸»è¦å‘ç°ï¼ˆæŒ‰ä¸»é¢˜ç»„ç»‡ï¼‰
3. åˆ†æå’Œç»“è®º
4. æ¥æºåˆ—è¡¨
"""

RESEARCHER_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ï¼Œè´Ÿè´£æ‰§è¡Œå…·ä½“çš„ä¿¡æ¯æœç´¢ä»»åŠ¡ã€‚ä»Šå¤©æ—¥æœŸ: {date}

## ä»»åŠ¡
ä½¿ç”¨æä¾›çš„æœç´¢å·¥å…·æ”¶é›†å…³äºæŒ‡å®šä¸»é¢˜çš„ä¿¡æ¯ã€‚

## å¯ç”¨å·¥å…·
1. **{search_tool_name}** - ç½‘ç»œæœç´¢ï¼Œè·å–æœ€æ–°ä¿¡æ¯
2. **think** - åæ€æœç´¢ç»“æœï¼Œè§„åˆ’ä¸‹ä¸€æ­¥
3. **write_file** - ä¿å­˜ç ”ç©¶å‘ç°åˆ°æ–‡ä»¶ã€‚æ ¼å¼: write_file("/research_ä¸»é¢˜.md", å†…å®¹)

## æ–‡ä»¶ç³»ç»Ÿ
- å·¥ä½œç›®å½•æ˜¯æ ¹ç›®å½• `/`
- å®Œæˆç ”ç©¶åï¼Œå°†ç ”ç©¶å‘ç°ä¿å­˜åˆ°æ–‡ä»¶
- æ–‡ä»¶åæ ¼å¼ï¼š`/research_ä¸»é¢˜å…³é”®è¯.md`

## å·¥ä½œæµç¨‹

### æœç´¢ç­–ç•¥
1. å…ˆè¿›è¡Œå¹¿æ³›æœç´¢ï¼Œäº†è§£ä¸»é¢˜æ¦‚å†µ
2. æ ¹æ®åˆæ­¥ç»“æœè¿›è¡Œé’ˆå¯¹æ€§æœç´¢
3. æ¯æ¬¡æœç´¢åç”¨ think è¯„ä¼°æ”¶è·

### æœç´¢é¢„ç®—
- ç®€å•æŸ¥è¯¢ï¼š2-3 æ¬¡æœç´¢
- å¤æ‚æŸ¥è¯¢ï¼šæœ€å¤š 5 æ¬¡æœç´¢
- æ‰¾åˆ° 3+ ä¸ªç›¸å…³æ¥æºåè€ƒè™‘åœæ­¢

### åœæ­¢æ¡ä»¶
- èƒ½å¤Ÿå…¨é¢å›ç­”ç ”ç©¶é—®é¢˜
- å·²æœ‰ 3 ä¸ªä»¥ä¸Šç›¸å…³æ¥æº
- æœ€è¿‘ 2 æ¬¡æœç´¢è¿”å›é‡å¤ä¿¡æ¯

## è¾“å‡ºè¦æ±‚
å®Œæˆæœç´¢åï¼š
1. æ•´ç†å‘ç°ï¼š
   - åˆ—å‡ºæ‰€æœ‰æŸ¥è¯¢å’Œå·¥å…·è°ƒç”¨
   - è¯¦ç»†è®°å½•æ‰€æœ‰å‘ç°ï¼ˆä¿ç•™åŸå§‹ä¿¡æ¯ï¼‰
   - ä¸ºæ¯ä¸ªæ¥æºæä¾›å®Œæ•´å¼•ç”¨
   - ä¸è¦ä¸¢å¤±ä»»ä½•ç›¸å…³ä¿¡æ¯
2. **å¿…é¡»ä½¿ç”¨ write_file ä¿å­˜ç ”ç©¶å‘ç°**ï¼š
   - æ ¼å¼ï¼š`write_file("/research_ä¸»é¢˜å…³é”®è¯.md", ç ”ç©¶å‘ç°å†…å®¹)`
   - ä¾‹å¦‚ï¼š`write_file("/research_deepseek.md", "# Deepseekç ”ç©¶å‘ç°\n...")`
3. è¿”å›ç®€è¦æ€»ç»“ç»™ä¸»ä»£ç†
"""

FINAL_REPORT_PROMPT = """åŸºäºæ‰€æœ‰æ”¶é›†çš„ç ”ç©¶å‘ç°ï¼Œåˆ›å»ºä¸€ä»½å…¨é¢çš„ç ”ç©¶æŠ¥å‘Šã€‚

ç ”ç©¶ç®€æŠ¥:
{research_brief}

ç ”ç©¶å‘ç°:
{findings}

ä»Šå¤©æ—¥æœŸ: {date}

## æŠ¥å‘Šè¦æ±‚

### ç»“æ„
1. ä½¿ç”¨æ¸…æ™°çš„æ ‡é¢˜å±‚çº§ï¼ˆ# æ ‡é¢˜ï¼Œ## ç« èŠ‚ï¼Œ### å°èŠ‚ï¼‰
2. æ ¹æ®å†…å®¹ç±»å‹é€‰æ‹©åˆé€‚çš„ç»“æ„ï¼š
   - æ¯”è¾ƒç±»ï¼šæ¦‚è¿° â†’ å„é¡¹ä»‹ç» â†’ å¯¹æ¯”åˆ†æ â†’ ç»“è®º
   - åˆ—è¡¨ç±»ï¼šç›´æ¥åˆ—è¡¨æˆ–åˆ†é¡¹ä»‹ç»
   - ç»¼è¿°ç±»ï¼šæ¦‚è¿° â†’ å„æ¦‚å¿µè¯¦è§£ â†’ æ€»ç»“

### å†…å®¹
1. ä½¿ç”¨ç®€æ´æ¸…æ™°çš„è¯­è¨€
2. åŒ…å«å…·ä½“äº‹å®å’Œè§è§£
3. å„ç« èŠ‚åº”è¶³å¤Ÿè¯¦ç»†
4. é€‚å½“ä½¿ç”¨é¡¹ç›®ç¬¦å·

### å¼•ç”¨è§„åˆ™
1. ä¸ºæ¯ä¸ª URL åˆ†é…å”¯ä¸€ç¼–å·
2. åœ¨æ–‡ä¸­ä½¿ç”¨ [1]ã€[2] ç­‰æ ‡æ³¨
3. æ–‡æœ«åˆ—å‡ºæ‰€æœ‰æ¥æºï¼š
   [1] æ¥æºæ ‡é¢˜: URL
   [2] æ¥æºæ ‡é¢˜: URL

### è¯­è¨€
- ä½¿ç”¨ä¸ç”¨æˆ·è¾“å…¥ç›¸åŒçš„è¯­è¨€æ’°å†™
- å¦‚æœç”¨æˆ·ä½¿ç”¨ä¸­æ–‡ï¼ŒæŠ¥å‘Šä½¿ç”¨ä¸­æ–‡
- å¦‚æœç”¨æˆ·ä½¿ç”¨è‹±æ–‡ï¼ŒæŠ¥å‘Šä½¿ç”¨è‹±æ–‡

## æ³¨æ„äº‹é¡¹
- ä¸è¦è‡ªæˆ‘å¼•ç”¨ï¼ˆä¸è¯´"æˆ‘"ã€"æœ¬æŠ¥å‘Š"ç­‰ï¼‰
- ä¸è¦è¯„è®ºè‡ªå·±åœ¨åšä»€ä¹ˆ
- ç›´æ¥å‘ˆç°ç ”ç©¶å†…å®¹
"""


# ============ å·¥å…·å®šä¹‰ ============

# åˆå§‹åŒ– Tavily å®¢æˆ·ç«¯
tavily_client = TavilyClient(api_key=TAVILY_API_KEY) if TAVILY_API_KEY else None
async_tavily_client = (
    AsyncTavilyClient(api_key=TAVILY_API_KEY) if TAVILY_API_KEY else None
)


@tool
def internet_search(
    query: str,
    max_results: int = MAX_SEARCH_RESULTS,
    topic: Literal["general", "news", "finance"] = "general",
    include_raw_content: bool = False,
) -> dict:
    """ä½¿ç”¨ Tavily æ‰§è¡Œç½‘ç»œæœç´¢ã€‚"""
    if tavily_client is None:
        return {
            "error": "Tavily API key not configured. Please set TAVILY_API_KEY in .env"
        }

    try:
        result = tavily_client.search(
            query,
            max_results=max_results,
            include_raw_content=include_raw_content,
            topic=topic,
        )

        formatted_results = []
        for i, item in enumerate(result.get("results", []), 1):
            formatted_results.append(
                f"--- æ¥æº {i}: {item.get('title', 'Unknown')} ---\n"
                f"URL: {item.get('url', 'N/A')}\n"
                f"æ‘˜è¦: {item.get('content', 'N/A')}\n"
            )

        return {
            "query": query,
            "results": "\n\n".join(formatted_results)
            if formatted_results
            else "æœªæ‰¾åˆ°ç»“æœ",
            "result_count": len(result.get("results", [])),
        }
    except Exception as e:
        return {"error": f"Tavily search failed: {str(e)}"}


@tool
def internet_search_bochai(
    query: str,
    count: int = MAX_SEARCH_RESULTS,
    summary: bool = True,
) -> dict:
    """ä½¿ç”¨åšæŸ¥æ¥å£æ‰§è¡Œç½‘ç»œæœç´¢ã€‚"""
    if not BOCHAI_API_KEY:
        return {
            "error": "BoChai API key not configured. Please set BOCHAI_API_KEY in .env"
        }

    headers = {
        "Authorization": f"{BOCHAI_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {"query": query, "summary": summary, "count": count}

    try:
        response = requests.post(
            BOCHAI_BASE_URL, headers=headers, json=payload, timeout=30
        )
        response.raise_for_status()
        data = response.json()
        items = data.get("data", {}).get("webPages", {}).get("value", [])

        formatted_results = []
        for i, item in enumerate(items, 1):
            formatted_results.append(
                f"--- æ¥æº {i}: {item.get('name', 'Unknown')} ---\n"
                f"URL: {item.get('url', 'N/A')}\n"
                f"æ‘˜è¦: {item.get('snippet', 'N/A')}\n"
            )

        return {
            "query": query,
            "results": "\n\n".join(formatted_results)
            if formatted_results
            else "æœªæ‰¾åˆ°ç»“æœ",
            "result_count": len(items),
        }
    except Exception as e:
        return {"error": f"BoChai search failed: {str(e)}"}


@tool
def think(reflection: str) -> str:
    """åæ€å·¥å…· - ç”¨äºæˆ˜ç•¥è§„åˆ’å’Œç»“æœåˆ†æã€‚

    åœ¨ä»¥ä¸‹æƒ…å†µä½¿ç”¨æ­¤å·¥å…·ï¼š
    1. åˆ†æç ”ç©¶é—®é¢˜ï¼Œè§„åˆ’ç ”ç©¶ç­–ç•¥
    2. è¯„ä¼°æœç´¢ç»“æœï¼Œå†³å®šä¸‹ä¸€æ­¥
    3. åˆ¤æ–­æ˜¯å¦å·²æ”¶é›†è¶³å¤Ÿä¿¡æ¯

    Args:
        reflection: ä½ çš„åæ€å’Œæ€è€ƒå†…å®¹

    Returns:
        ç¡®è®¤åæ€å·²è®°å½•
    """
    return f"åæ€å·²è®°å½•: {reflection}"


@tool
def get_current_time() -> str:
    """è·å–å½“å‰æ—¶é—´ã€‚æ¶‰åŠåˆ°æ—¶é—´æœ‰å…³çš„æ“ä½œï¼Œéƒ½éœ€è¦å…ˆæŸ¥è¯¢æ—¶é—´"""
    now = datetime.now()
    return f"å½“å‰æ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')}"


# ============ ç ”ç©¶å­ä»£ç†å®šä¹‰ ============
search_tool = internet_search_bochai if USE_BOCHAI_SEARCH else internet_search
# StructuredTool æ²¡æœ‰ __name__ï¼Œä½¿ç”¨ name å±æ€§ï¼›å‡½æ•°æœ¬èº«ä¹Ÿæ”¯æŒ __name__ ä½œä¸ºå¤‡é€‰
search_tool_name = getattr(search_tool, "name", None) or getattr(
    search_tool, "__name__", "internet_search"
)

researcher_subagent = {
    "name": "researcher",
    "description": """ä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ï¼Œç”¨äºæ‰§è¡Œç½‘ç»œæœç´¢å’Œä¿¡æ¯æ”¶é›†ã€‚
    
å½“éœ€è¦æœç´¢äº’è”ç½‘è·å–ä¿¡æ¯æ—¶ä½¿ç”¨æ­¤ä»£ç†ã€‚å®ƒä¼šï¼š
1. æ‰§è¡Œå¤šæ¬¡æœç´¢ä»¥å…¨é¢è¦†ç›–ä¸»é¢˜
2. æ•´ç†å’Œæ ¼å¼åŒ–æœç´¢ç»“æœ
3. æä¾›å¸¦å¼•ç”¨çš„ç ”ç©¶å‘ç°""",
    "system_prompt": RESEARCHER_SYSTEM_PROMPT.format(
        date=get_today_str(),
        search_tool_name=search_tool_name,
    ),
    "tools": [search_tool, think],
}


# ============ åˆ›å»ºæ¨¡å‹å’Œä»£ç† ============

# ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£è°ƒç”¨æ¨¡å‹
model = init_chat_model(
    model=f"openai:{OPENAI_MODEL}",
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL,
    temperature=LLM_TEMPERATURE,
    max_tokens=LLM_MAX_TOKENS,
)

# ä¸»ä»£ç†ç³»ç»Ÿæç¤º
main_system_prompt = SUPERVISOR_SYSTEM_PROMPT.format(
    date=get_today_str(),
    max_concurrent=MAX_CONCURRENT_RESEARCH,
    max_iterations=6,
)

# åˆ›å»º FilesystemBackendï¼ˆç”¨äºä¿å­˜ç ”ç©¶æŠ¥å‘Šï¼‰
filesystem_backend = FilesystemBackend(
    root_dir=WORKSPACE_DIR,
    virtual_mode=True,
)

# åˆ›å»º checkpointer
checkpointer = MemorySaver()

# åˆ›å»º Open Deep Research Agent
agent = create_deep_agent_customized(
    model=model,
    tools=[think, get_current_time],  # ä¸»ä»£ç†å·¥å…·
    system_prompt=main_system_prompt,
    checkpointer=checkpointer,
    backend=filesystem_backend,
    subagents=[researcher_subagent],  # ç ”ç©¶å­ä»£ç†
    # å‹ç¼©å‚æ•°
    max_context_window=DEFAULT_CONTEXT_LIMIT,
    max_output_tokens=LLM_MAX_TOKENS,
    compression_trigger=0.80,
    messages_to_keep=5,
)


# ============ è¿è¡Œé…ç½® ============
SHOW_SUBAGENT_DETAILS = True
USE_SYNC_MODE = False
STREAM_MODE = "tokens"


# ============ å“åº”å‡½æ•° ============


def sync_response(user_input: str, config: dict):
    """åŒæ­¥è°ƒç”¨ agent"""
    result = agent.invoke(
        {"messages": [{"role": "user", "content": user_input}]},
        config=config,
    )

    for msg in result.get("messages", []):
        msg_type = getattr(msg, "type", type(msg).__name__)
        content = getattr(msg, "content", "")

        if msg_type == "human":
            continue

        if msg_type == "ai":
            if content:
                print(f"\nğŸ’¬ AI: {content}")

            tool_calls = getattr(msg, "tool_calls", [])
            if tool_calls:
                for tc in tool_calls:
                    print(
                        f"\nğŸ”§ å·¥å…·è°ƒç”¨: {tc.get('name', 'unknown')} | å‚æ•°: {tc.get('args', {})}"
                    )

        elif msg_type == "tool":
            tool_name = getattr(msg, "name", "unknown")
            display_content = (
                str(content)[:500] + "..." if len(str(content)) > 500 else content
            )
            print(f"\nğŸ“¦ [{tool_name}]: {display_content}")


async def stream_tokens_response(user_input: str, config: dict):
    """Token çº§åˆ«æµå¼è¾“å‡º"""
    print("\nğŸ’¬ AI: ", end="", flush=True)

    pending_tool_calls: dict[str, dict] = {}
    printed_tool_calls: set[str] = set()

    try:
        async for msg_chunk, metadata in agent.astream(
            {"messages": [{"role": "user", "content": user_input}]},
            config=config,
            stream_mode="messages",
        ):
            msg_type = getattr(msg_chunk, "type", type(msg_chunk).__name__)

            if msg_type == "AIMessageChunk":
                content = getattr(msg_chunk, "content", "")
                if content:
                    print(content, end="", flush=True)

                tool_call_chunks = getattr(msg_chunk, "tool_call_chunks", [])
                for tc in tool_call_chunks:
                    tc_id = tc.get("id") or str(tc.get("index", 0))
                    if tc_id not in pending_tool_calls:
                        pending_tool_calls[tc_id] = {"name": "", "args": ""}
                    if tc.get("name"):
                        pending_tool_calls[tc_id]["name"] = tc["name"]
                    if tc.get("args"):
                        pending_tool_calls[tc_id]["args"] += tc["args"]

                tool_calls = getattr(msg_chunk, "tool_calls", [])
                for tc in tool_calls:
                    tc_id = tc.get("id", "")
                    name = tc.get("name", "")
                    args = tc.get("args", {})

                    if name and tc_id not in printed_tool_calls:
                        printed_tool_calls.add(tc_id)
                        if isinstance(args, dict) and args:
                            args_items = []
                            for k, v in args.items():
                                v_str = (
                                    repr(v)
                                    if len(repr(v)) < 50
                                    else repr(v)[:47] + "..."
                                )
                                args_items.append(f"{k}={v_str}")
                            args_str = ", ".join(args_items)
                            print(
                                f"\n\nğŸ”§ å·¥å…·è°ƒç”¨: {name}\n   å‚æ•°: {args_str}",
                                flush=True,
                            )
                        else:
                            print(f"\n\nğŸ”§ å·¥å…·è°ƒç”¨: {name}", flush=True)

            elif msg_type == "tool":
                tool_name = getattr(msg_chunk, "name", "unknown")
                content = getattr(msg_chunk, "content", "")
                display_content = (
                    str(content)[:500] + "..." if len(str(content)) > 500 else content
                )
                print(f"\n\nğŸ“¦ [{tool_name}]: {display_content}", flush=True)
                print("\nğŸ’¬ AI: ", end="", flush=True)

        print()
    except Exception as e:
        error_str = str(e).lower()
        if "connection" in error_str or "timeout" in error_str:
            print(f"\n\nâš ï¸ ç½‘ç»œè¿æ¥é—®é¢˜: {e}")
        else:
            print(f"\n\nâŒ é”™è¯¯: {e}")


async def stream_response(user_input: str, config: dict):
    """èŠ‚ç‚¹çº§åˆ«æµå¼è¾“å‡º"""
    try:
        if SHOW_SUBAGENT_DETAILS:
            async for chunk in agent.astream(
                {"messages": [{"role": "user", "content": user_input}]},
                config=config,
                stream_mode="updates",
                subgraphs=True,
            ):
                namespace, update = chunk
                is_subagent = len(namespace) > 0
                prefix = "    ğŸ”¹ [ç ”ç©¶å­ä»£ç†] " if is_subagent else ""

                if "model" in update:
                    messages = update["model"].get("messages", [])
                    for msg in messages:
                        msg_type = getattr(msg, "type", type(msg).__name__)
                        content = getattr(msg, "content", "")

                        if msg_type == "ai":
                            if content:
                                print(f"\n{prefix}ğŸ’¬ AI: {content}")

                            tool_calls = getattr(msg, "tool_calls", [])
                            if tool_calls:
                                for tc in tool_calls:
                                    print(
                                        f"\n{prefix}ğŸ”§ å·¥å…·è°ƒç”¨: {tc.get('name', 'unknown')} | å‚æ•°: {tc.get('args', {})}"
                                    )

                if "tools" in update:
                    messages = update["tools"].get("messages", [])
                    for msg in messages:
                        tool_name = getattr(msg, "name", "unknown")
                        content = getattr(msg, "content", "")
                        display_content = (
                            str(content)[:500] + "..."
                            if len(str(content)) > 500
                            else content
                        )
                        print(f"\n{prefix}ğŸ“¦ [{tool_name}]: {display_content}")
        else:
            async for chunk in agent.astream(
                {"messages": [{"role": "user", "content": user_input}]},
                config=config,
                stream_mode="values",
            ):
                if "messages" in chunk:
                    msg = chunk["messages"][-1]
                    msg_type = getattr(msg, "type", type(msg).__name__)
                    content = getattr(msg, "content", "")

                    if msg_type == "ai":
                        if content:
                            print(f"\nğŸ’¬ AI: {content}")

                        tool_calls = getattr(msg, "tool_calls", [])
                        if tool_calls:
                            for tc in tool_calls:
                                print(
                                    f"\nğŸ”§ å·¥å…·è°ƒç”¨: {tc.get('name', 'unknown')} | å‚æ•°: {tc.get('args', {})}"
                                )

                    elif msg_type == "tool":
                        tool_name = getattr(msg, "name", "unknown")
                        display_content = (
                            str(content)[:500] + "..."
                            if len(str(content)) > 500
                            else content
                        )
                        print(f"\nğŸ“¦ [{tool_name}]: {display_content}")
    except Exception as e:
        error_msg = str(e)
        if "tool_calls" in error_msg and "tool messages" in error_msg:
            print("\nâš ï¸ å¯¹è¯å†å²æŸåï¼Œè¯·è¾“å…¥ 'new' å¼€å§‹æ–°å¯¹è¯")
        else:
            raise


# ============ ä¸»å¾ªç¯ ============


async def chat_loop():
    """äº¤äº’å¼ç ”ç©¶å¯¹è¯"""
    global SHOW_SUBAGENT_DETAILS, USE_SYNC_MODE, STREAM_MODE

    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}

    print("=" * 70)
    print("ğŸ”¬ Open Deep Research Agent (åŸºäº DeepAgents)")
    print("=" * 70)
    print(f"ğŸ“Œ å¯¹è¯çº¿ç¨‹ ID: {thread_id}")
    print(f"ğŸ“ å·¥ä½œç›®å½•: {WORKSPACE_DIR}")
    print(f"ğŸ”§ æ¨¡å‹: {OPENAI_MODEL}")
    print(f"ğŸ“Š ä¸Šä¸‹æ–‡çª—å£: {DEFAULT_CONTEXT_LIMIT:,} tokens")
    print(f"ğŸ” æœç´¢ç»“æœæ•°: {MAX_SEARCH_RESULTS}")
    print(f"ğŸŒ æœç´¢å·¥å…·: {'BoChai' if USE_BOCHAI_SEARCH else 'Tavily'}")
    print("-" * 70)
    print("ğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("   - è¾“å…¥ç ”ç©¶é—®é¢˜ï¼ŒAI å°†è‡ªåŠ¨è§„åˆ’å¹¶æ‰§è¡Œæ·±åº¦ç ”ç©¶")
    print("   - æ”¯æŒä¸­è‹±æ–‡è¾“å…¥")
    print("   - ç ”ç©¶æŠ¥å‘Šå°†è‡ªåŠ¨ç”Ÿæˆå¸¦å¼•ç”¨çš„ç»“æ„åŒ–å†…å®¹")
    print("-" * 70)
    print("ğŸ’¡ å‘½ä»¤:")
    print("   'quit' / 'exit' - é€€å‡º")
    print("   'new' - å¼€å§‹æ–°å¯¹è¯")
    print("   'toggle' - åˆ‡æ¢å­ä»£ç†ç»†èŠ‚æ˜¾ç¤º")
    print("   'sync' - åˆ‡æ¢åŒæ­¥/æµå¼æ¨¡å¼")
    print("   'stream' - åˆ‡æ¢ token/èŠ‚ç‚¹çº§åˆ«æµå¼")
    print("=" * 70)

    # ç¤ºä¾‹æç¤º
    print("\n ç¤ºä¾‹ç ”ç©¶é—®é¢˜:")
    print("   1. æ¯”è¾ƒ GPT-5.1 å’Œ Claude 4.5 opus åœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›")
    print("   2. 2025å¹´äººå·¥æ™ºèƒ½é¢†åŸŸæœ‰å“ªäº›é‡å¤§çªç ´ï¼Ÿ")
    print()

    while True:
        try:
            user_input = input("\nğŸ‘¤ ç ”ç©¶é—®é¢˜: ").strip()

            if not user_input:
                continue

            if user_input.lower() in ["quit", "exit", "q"]:
                print("\nğŸ‘‹ å†è§ï¼")
                break

            if user_input.lower() == "new":
                thread_id = str(uuid.uuid4())
                config = {"configurable": {"thread_id": thread_id}}
                print(f"\nğŸ”„ å·²å¼€å§‹æ–°å¯¹è¯ï¼Œçº¿ç¨‹ ID: {thread_id}")
                continue

            if user_input.lower() == "toggle":
                SHOW_SUBAGENT_DETAILS = not SHOW_SUBAGENT_DETAILS
                status = "å¼€å¯" if SHOW_SUBAGENT_DETAILS else "å…³é—­"
                print(f"\nğŸ”„ å­ä»£ç†ç»†èŠ‚æ˜¾ç¤ºå·²{status}")
                continue

            if user_input.lower() == "sync":
                USE_SYNC_MODE = not USE_SYNC_MODE
                mode = "åŒæ­¥" if USE_SYNC_MODE else "æµå¼"
                print(f"\nğŸ”„ å·²åˆ‡æ¢åˆ°{mode}æ¨¡å¼")
                continue

            if user_input.lower() == "stream":
                STREAM_MODE = "nodes" if STREAM_MODE == "tokens" else "tokens"
                mode_name = "Tokençº§åˆ«" if STREAM_MODE == "tokens" else "èŠ‚ç‚¹çº§åˆ«"
                print(f"\nğŸ”„ å·²åˆ‡æ¢åˆ°{mode_name}æµå¼æ¨¡å¼")
                continue

            print("\n" + "=" * 70)
            print("ğŸ”¬ å¼€å§‹æ·±åº¦ç ”ç©¶...")
            print("=" * 70)

            if USE_SYNC_MODE:
                sync_response(user_input, config)
            elif STREAM_MODE == "tokens":
                await stream_tokens_response(user_input, config)
            else:
                await stream_response(user_input, config)

            print("\n" + "=" * 70)
            print("âœ… ç ”ç©¶å®Œæˆ")
            print("=" * 70)

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            import traceback

            traceback.print_exc()


# ============ å¿«é€Ÿç ”ç©¶å‡½æ•° ============


async def quick_research(question: str, save_to_file: bool = True) -> str:
    """å¿«é€Ÿæ‰§è¡Œä¸€æ¬¡ç ”ç©¶å¹¶è¿”å›æŠ¥å‘Š

    Args:
        question: ç ”ç©¶é—®é¢˜
        save_to_file: æ˜¯å¦å°†æŠ¥å‘Šä¿å­˜åˆ°æ–‡ä»¶

    Returns:
        ç ”ç©¶æŠ¥å‘Šå†…å®¹
    """
    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}

    print(f"ğŸ”¬ å¼€å§‹ç ”ç©¶: {question}")
    print("-" * 50)

    result = agent.invoke(
        {"messages": [{"role": "user", "content": question}]},
        config=config,
    )

    # è·å–æœ€ç»ˆå›å¤
    final_response = ""
    for msg in result.get("messages", []):
        msg_type = getattr(msg, "type", type(msg).__name__)
        if msg_type == "ai":
            content = getattr(msg, "content", "")
            if content:
                final_response = content

    if save_to_file and final_response:
        # ç”Ÿæˆæ–‡ä»¶å
        safe_name = "".join(
            c if c.isalnum() or c in " _-" else "_" for c in question[:50]
        )
        filename = f"research_{safe_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        filepath = os.path.join(WORKSPACE_DIR, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(f"# ç ”ç©¶æŠ¥å‘Š: {question}\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("---\n\n")
            f.write(final_response)

        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath}")

    return final_response


if __name__ == "__main__":
    asyncio.run(chat_loop())
