"""
å®Œæ•´ç¤ºä¾‹ï¼šä½¿ç”¨è‡ªå®šä¹‰ DeepAgents
- æ”¯æŒå¤šè½®è¿ç»­å¯¹è¯
- ä½¿ç”¨ AU2 8æ®µå¼ç»“æ„åŒ–å‹ç¼©ï¼ˆæ›´é€‚åˆä»£ç å¼€å‘åœºæ™¯ï¼‰
- é›†æˆ Tavily æœç´¢
- FilesystemBackend çœŸå®æ–‡ä»¶è¯»å†™
- SubAgent å­ä»£ç†
- ä» .env è¯»å–é…ç½®
"""

import os
import uuid
from typing import Literal
from dotenv import load_dotenv

from tavily import TavilyClient
from langchain.chat_models import init_chat_model
from langchain_core.tools import tool
from langgraph.checkpoint.memory import MemorySaver
from deepagents.backends import FilesystemBackend

from agent_templates import create_deep_agent_customized

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# ============ ä» .env è¯»å–é…ç½® ============
# API é…ç½®
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "deepseek-chat")

# LLM å‚æ•°
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.7"))
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "8192"))

# Agent é…ç½®
DEFAULT_CONTEXT_LIMIT = int(os.getenv("DEFAULT_CONTEXT_LIMIT", "128000"))

# Tavily é…ç½®
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

# å·¥ä½œç›®å½•ï¼ˆç”¨äºæ–‡ä»¶æ“ä½œï¼‰
# æ³¨æ„ï¼šç›´æ¥ä½¿ç”¨ "temp"ï¼Œä¸è¦å¸¦ "./"
WORKSPACE_DIR = os.path.join(os.path.dirname(__file__), "temp")
os.makedirs(WORKSPACE_DIR, exist_ok=True)

# åˆå§‹åŒ– Tavily å®¢æˆ·ç«¯
tavily_client = TavilyClient(api_key=TAVILY_API_KEY) if TAVILY_API_KEY else None


# ç½‘ç»œæœç´¢å·¥å…·
def internet_search(
    query: str,
    max_results: int = 5,
    topic: Literal["general", "news", "finance"] = "general",
    include_raw_content: bool = False,
):
    """è¿è¡Œç½‘ç»œæœç´¢ã€‚

    Args:
        query: æœç´¢æŸ¥è¯¢
        max_results: æœ€å¤§ç»“æœæ•°é‡
        topic: æœç´¢ä¸»é¢˜ç±»å‹ (general/news/finance)
        include_raw_content: æ˜¯å¦åŒ…å«åŸå§‹å†…å®¹
    """
    if tavily_client is None:
        return {"error": "Tavily API key not configured"}
    return tavily_client.search(
        query,
        max_results=max_results,
        include_raw_content=include_raw_content,
        topic=topic,
    )


# ============ è‡ªå®šä¹‰å·¥å…· ============


@tool
def get_current_time() -> str:
    """è·å–å½“å‰æ—¶é—´ã€‚æ¶‰åŠåˆ°æ—¶é—´æœ‰å…³çš„æ“ä½œï¼Œéƒ½éœ€è¦å…ˆæŸ¥è¯¢æ—¶é—´"""
    from datetime import datetime

    now = datetime.now()
    return f"å½“å‰æ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')}"


# ============ å®šä¹‰å­ä»£ç† ============
# ç ”ç©¶å­ä»£ç†ï¼šä¸“é—¨ç”¨äºç½‘ç»œæœç´¢
research_subagent = {
    "name": "researcher",
    "description": "ä¸“é—¨ç”¨äºç½‘ç»œæœç´¢å’Œç ”ç©¶é—®é¢˜çš„å­ä»£ç†ã€‚å½“éœ€è¦æœç´¢äº’è”ç½‘è·å–æœ€æ–°ä¿¡æ¯æ—¶ä½¿ç”¨ã€‚",
    "system_prompt": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. ä½¿ç”¨ internet_search å·¥å…·æœç´¢ç›¸å…³ä¿¡æ¯
2. æ•´ç†å¹¶æ€»ç»“æœç´¢ç»“æœ
3. æä¾›æ¸…æ™°ã€å‡†ç¡®çš„ç ”ç©¶æŠ¥å‘Š

è¯·å§‹ç»ˆä¿æŒå®¢è§‚å’Œå‡†ç¡®ã€‚""",
    "tools": [internet_search],
}


# ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£è°ƒç”¨æ¨¡å‹ï¼ˆä» .env è¯»å–é…ç½®ï¼‰
model = init_chat_model(
    model=f"openai:{OPENAI_MODEL}",
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL,
    temperature=LLM_TEMPERATURE,
    max_tokens=LLM_MAX_TOKENS,
)

# ç³»ç»Ÿæç¤º
system_prompt = """ä½ æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ AI åŠ©æ‰‹ï¼Œå…·æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š

## æ ¸å¿ƒèƒ½åŠ›
1. **ä»»åŠ¡è§„åˆ’**: ä½¿ç”¨ write_todos å·¥å…·æ¥è§„åˆ’å’Œè·Ÿè¸ªå¤æ‚ä»»åŠ¡
2. **æ–‡ä»¶ç®¡ç†**: ä½¿ç”¨ ls, read_file, write_file, edit_file å·¥å…·ç®¡ç†çœŸå®æ–‡ä»¶ç³»ç»Ÿ
3. **å­ä»£ç†è°ƒç”¨**: ä½¿ç”¨ task å·¥å…·è°ƒç”¨ä¸“é—¨çš„å­ä»£ç†ï¼š
   - `researcher`: ç”¨äºç½‘ç»œæœç´¢å’Œç ”ç©¶
4. **æ—¶é—´æŸ¥è¯¢**: ä½¿ç”¨ get_current_time è·å–å½“å‰æ—¶é—´

## æ–‡ä»¶ç³»ç»Ÿ
- å½“å‰ç›®å½•å°±æ˜¯å·¥ä½œç›®å½•ï¼ˆæ ¹ç›®å½• /ï¼‰
- ç›´æ¥ä½¿ç”¨æ–‡ä»¶åä¿å­˜ï¼Œå¦‚ write_file("/report.md", content)
- ä¸è¦å†åˆ›å»º workspace å­ç›®å½•
- ä½¿ç”¨ ls / æŸ¥çœ‹å½“å‰ç›®å½•å†…å®¹

## å·¥ä½œæµç¨‹
1. æ”¶åˆ°å¤æ‚ä»»åŠ¡æ—¶ï¼Œå…ˆä½¿ç”¨ write_todos åˆ¶å®šè®¡åˆ’
2. æŒ‰è®¡åˆ’é€æ­¥æ‰§è¡Œï¼Œæ›´æ–°ä»»åŠ¡çŠ¶æ€
3. éœ€è¦ç½‘ç»œæœç´¢æ—¶ï¼Œè°ƒç”¨ researcher å­ä»£ç†
4. éœ€è¦ä¿å­˜ç»“æœæ—¶ï¼Œç›´æ¥å†™å…¥æ ¹ç›®å½•
"""

# åˆ›å»º FilesystemBackendï¼ˆçœŸå®æ–‡ä»¶ç³»ç»Ÿï¼‰
filesystem_backend = FilesystemBackend(
    root_dir=WORKSPACE_DIR,
    virtual_mode=True,  # å®‰å…¨æ¨¡å¼ï¼Œé™åˆ¶åœ¨ workspace ç›®å½•å†…
)

# åˆ›å»º checkpointer ç”¨äºä¿å­˜å¯¹è¯çŠ¶æ€
checkpointer = MemorySaver()

# åˆ›å»ºè‡ªå®šä¹‰ deep agent
agent = create_deep_agent_customized(
    model=model,
    tools=[get_current_time],  # ä¸»ä»£ç†çš„å·¥å…·
    system_prompt=system_prompt,
    checkpointer=checkpointer,  # å¯ç”¨å¯¹è¯è®°å¿†
    backend=filesystem_backend,  # çœŸå®æ–‡ä»¶ç³»ç»Ÿ
    subagents=[research_subagent],  # å­ä»£ç†
    # AU2 å‹ç¼©å‚æ•°ï¼ˆä» .env è¯»å–ï¼‰
    max_context_window=DEFAULT_CONTEXT_LIMIT,  # æ¨¡å‹è¾“å…¥ä¸Šé™
    max_output_tokens=LLM_MAX_TOKENS,  # æ¨¡å‹è¾“å‡ºä¸Šé™
    compression_trigger=0.80,  # 80% æ—¶è§¦å‘å‹ç¼©
    messages_to_keep=5,  # ä¿ç•™æœ€è¿‘ 5 æ¡æ¶ˆæ¯
)


# æ˜¯å¦æ˜¾ç¤ºå­æ™ºèƒ½ä½“å†…éƒ¨ç»†èŠ‚
SHOW_SUBAGENT_DETAILS = True

# æ˜¯å¦ä½¿ç”¨åŒæ­¥æ¨¡å¼ï¼ˆinvoke è€Œä¸æ˜¯ astreamï¼‰
USE_SYNC_MODE = False

# æµå¼æ¨¡å¼: "tokens" = tokençº§åˆ«æµå¼, "nodes" = èŠ‚ç‚¹çº§åˆ«æµå¼
STREAM_MODE = "tokens"


# åŒæ­¥æ¨¡å¼å“åº”
def sync_response(user_input: str, config: dict):
    """åŒæ­¥è°ƒç”¨ agentï¼Œç­‰å¾…å®Œæ•´å“åº”"""
    result = agent.invoke(
        {"messages": [{"role": "user", "content": user_input}]},
        config=config,
    )

    # æ‰“å°æ‰€æœ‰æ¶ˆæ¯
    for msg in result.get("messages", []):
        msg_type = getattr(msg, "type", type(msg).__name__)
        content = getattr(msg, "content", "")

        if msg_type == "human":
            continue  # è·³è¿‡ç”¨æˆ·æ¶ˆæ¯

        if msg_type == "ai":
            if content:
                print(f"\nğŸ’¬ AI: {content}")

            tool_calls = getattr(msg, "tool_calls", [])
            if tool_calls:
                for tc in tool_calls:
                    print(
                        f"\nğŸ”§ è°ƒç”¨å·¥å…·: {tc.get('name', 'unknown')} | å‚æ•°: {tc.get('args', {})}"
                    )

        elif msg_type == "tool":
            tool_name = getattr(msg, "name", "unknown")
            display_content = (
                str(content)[:300] + "..." if len(str(content)) > 300 else content
            )
            print(f"\nğŸ“¦ [{tool_name}]: {display_content}")


# Token çº§åˆ«æµå¼è¾“å‡º
async def stream_tokens_response(user_input: str, config: dict):
    """Token çº§åˆ«æµå¼è¾“å‡ºï¼Œæ¯ä¸ª token å®æ—¶æ˜¾ç¤º"""
    print("\nğŸ’¬ AI: ", end="", flush=True)

    # ç”¨äºç´¯ç§¯å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆå› ä¸ºå‚æ•°æ˜¯åˆ†å—ä¼ æ¥çš„ï¼‰
    pending_tool_calls: dict[str, dict] = {}
    printed_tool_calls: set[str] = set()  # å·²æ‰“å°çš„å·¥å…·è°ƒç”¨

    try:
        async for msg_chunk, metadata in agent.astream(
            {"messages": [{"role": "user", "content": user_input}]},
            config=config,
            stream_mode="messages",
        ):
            msg_type = getattr(msg_chunk, "type", type(msg_chunk).__name__)

            if msg_type == "AIMessageChunk":
                content = getattr(msg_chunk, "content", "")
                if content:
                    print(content, end="", flush=True)

                # ç´¯ç§¯å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆå‚æ•°æ˜¯åˆ†å—ä¼ æ¥çš„ï¼‰
                tool_call_chunks = getattr(msg_chunk, "tool_call_chunks", [])
                for tc in tool_call_chunks:
                    tc_id = tc.get("id") or str(tc.get("index", 0))
                    if tc_id not in pending_tool_calls:
                        pending_tool_calls[tc_id] = {"name": "", "args": ""}
                    if tc.get("name"):
                        pending_tool_calls[tc_id]["name"] = tc["name"]
                    if tc.get("args"):
                        pending_tool_calls[tc_id]["args"] += tc["args"]

                # æ£€æŸ¥å®Œæ•´çš„å·¥å…·è°ƒç”¨
                tool_calls = getattr(msg_chunk, "tool_calls", [])
                for tc in tool_calls:
                    tc_id = tc.get("id", "")
                    name = tc.get("name", "")
                    args = tc.get("args", {})

                    # åªæ‰“å°æœ‰åå­—ä¸”æœªæ‰“å°è¿‡çš„å·¥å…·è°ƒç”¨
                    if name and tc_id not in printed_tool_calls:
                        printed_tool_calls.add(tc_id)
                        # æ ¼å¼åŒ–å‚æ•°æ˜¾ç¤º
                        if isinstance(args, dict) and args:
                            args_items = []
                            for k, v in args.items():
                                v_str = (
                                    repr(v)
                                    if len(repr(v)) < 50
                                    else repr(v)[:47] + "..."
                                )
                                args_items.append(f"{k}={v_str}")
                            args_str = ", ".join(args_items)
                            print(
                                f"\n\nğŸ”§ è°ƒç”¨å·¥å…·: {name}\n   å‚æ•°: {args_str}",
                                flush=True,
                            )
                        else:
                            print(f"\n\nğŸ”§ è°ƒç”¨å·¥å…·: {name}", flush=True)

            elif msg_type == "tool":
                tool_name = getattr(msg_chunk, "name", "unknown")
                content = getattr(msg_chunk, "content", "")
                display_content = (
                    str(content)[:300] + "..." if len(str(content)) > 300 else content
                )
                print(f"\n\nğŸ“¦ [{tool_name}]: {display_content}", flush=True)
                print("\nğŸ’¬ AI: ", end="", flush=True)

        print()
    except Exception as e:
        error_str = str(e).lower()
        if "connection" in error_str or "timeout" in error_str:
            print(f"\n\nâš ï¸ ç½‘ç»œè¿æ¥é—®é¢˜: {e}")
        else:
            print(f"\n\nâŒ Token æµå¼é”™è¯¯: {e}")


# èŠ‚ç‚¹çº§åˆ«æµå¼è¾“å‡º
async def stream_response(user_input: str, config: dict):
    """æµå¼è¾“å‡º agent å“åº”ï¼Œæ”¯æŒæ˜¾ç¤ºå­æ™ºèƒ½ä½“å†…éƒ¨ç»†èŠ‚"""

    try:
        if SHOW_SUBAGENT_DETAILS:
            async for chunk in agent.astream(
                {"messages": [{"role": "user", "content": user_input}]},
                config=config,
                stream_mode="updates",
                subgraphs=True,
            ):
                namespace, update = chunk
                is_subagent = len(namespace) > 0
                prefix = "    ğŸ”¹ [å­ä»£ç†] " if is_subagent else ""

                if "model" in update:
                    messages = update["model"].get("messages", [])
                    for msg in messages:
                        msg_type = getattr(msg, "type", type(msg).__name__)
                        content = getattr(msg, "content", "")

                        if msg_type == "ai":
                            if content:
                                print(f"\n{prefix}ğŸ’¬ AI: {content}")

                            tool_calls = getattr(msg, "tool_calls", [])
                            if tool_calls:
                                for tc in tool_calls:
                                    print(
                                        f"\n{prefix}ğŸ”§ è°ƒç”¨å·¥å…·: {tc.get('name', 'unknown')} | å‚æ•°: {tc.get('args', {})}"
                                    )

                if "tools" in update:
                    messages = update["tools"].get("messages", [])
                    for msg in messages:
                        tool_name = getattr(msg, "name", "unknown")
                        content = getattr(msg, "content", "")
                        display_content = (
                            str(content)[:300] + "..."
                            if len(str(content)) > 300
                            else content
                        )
                        print(f"\n{prefix}ğŸ“¦ [{tool_name}]: {display_content}")
        else:
            async for chunk in agent.astream(
                {"messages": [{"role": "user", "content": user_input}]},
                config=config,
                stream_mode="values",
            ):
                if "messages" in chunk:
                    msg = chunk["messages"][-1]
                    msg_type = getattr(msg, "type", type(msg).__name__)
                    content = getattr(msg, "content", "")

                    if msg_type == "ai":
                        if content:
                            print(f"\nğŸ’¬ AI: {content}")

                        tool_calls = getattr(msg, "tool_calls", [])
                        if tool_calls:
                            for tc in tool_calls:
                                print(
                                    f"\nğŸ”§ è°ƒç”¨å·¥å…·: {tc.get('name', 'unknown')} | å‚æ•°: {tc.get('args', {})}"
                                )

                    elif msg_type == "tool":
                        tool_name = getattr(msg, "name", "unknown")
                        display_content = (
                            str(content)[:300] + "..."
                            if len(str(content)) > 300
                            else content
                        )
                        print(f"\nğŸ“¦ [{tool_name}]: {display_content}")
    except Exception as e:
        error_msg = str(e)
        if "tool_calls" in error_msg and "tool messages" in error_msg:
            print("\nâš ï¸ å¯¹è¯å†å²æŸåï¼Œè¯·è¾“å…¥ 'new' å¼€å§‹æ–°å¯¹è¯")
        else:
            raise


# å¤šè½®å¯¹è¯ä¸»å¾ªç¯
async def chat_loop():
    """äº¤äº’å¼å¤šè½®å¯¹è¯"""
    global SHOW_SUBAGENT_DETAILS, USE_SYNC_MODE, STREAM_MODE

    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}

    print("=" * 60)
    print("ğŸ¤– DeepAgents å¤šè½®å¯¹è¯æ¨¡å¼ (è‡ªå®šä¹‰ç‰ˆ)")
    print("=" * 60)
    print(f"ğŸ“Œ å¯¹è¯çº¿ç¨‹ ID: {thread_id}")
    print(f"ğŸ“ å·¥ä½œç›®å½•: {WORKSPACE_DIR}")
    print(f"ğŸ”§ æ¨¡å‹: {OPENAI_MODEL}")
    print(f"ğŸ“Š ä¸Šä¸‹æ–‡çª—å£: {DEFAULT_CONTEXT_LIMIT:,} tokens")
    print(f"ğŸ“Š æœ€å¤§è¾“å‡º: {LLM_MAX_TOKENS:,} tokens")
    print(f"ğŸŒ¡ï¸ Temperature: {LLM_TEMPERATURE}")
    print("-" * 60)
    print("ğŸ’¡ è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºå¯¹è¯")
    print("ğŸ’¡ è¾“å…¥ 'new' å¼€å§‹æ–°å¯¹è¯")
    print("ğŸ’¡ è¾“å…¥ 'toggle' åˆ‡æ¢å­ä»£ç†ç»†èŠ‚æ˜¾ç¤º")
    print("ğŸ’¡ è¾“å…¥ 'sync' åˆ‡æ¢åŒæ­¥/æµå¼æ¨¡å¼")
    print("ğŸ’¡ è¾“å…¥ 'stream' åˆ‡æ¢ token/èŠ‚ç‚¹ çº§åˆ«æµå¼")
    print("=" * 60)

    while True:
        try:
            user_input = input("\nğŸ‘¤ ä½ : ").strip()

            if not user_input:
                continue

            if user_input.lower() in ["quit", "exit", "q"]:
                print("\nğŸ‘‹ å†è§ï¼")
                break

            if user_input.lower() == "new":
                thread_id = str(uuid.uuid4())
                config = {"configurable": {"thread_id": thread_id}}
                print(f"\nğŸ”„ å·²å¼€å§‹æ–°å¯¹è¯ï¼Œçº¿ç¨‹ ID: {thread_id}")
                continue

            if user_input.lower() == "toggle":
                SHOW_SUBAGENT_DETAILS = not SHOW_SUBAGENT_DETAILS
                status = "å¼€å¯" if SHOW_SUBAGENT_DETAILS else "å…³é—­"
                print(f"\nğŸ”„ å­ä»£ç†ç»†èŠ‚æ˜¾ç¤ºå·²{status}")
                continue

            if user_input.lower() == "sync":
                USE_SYNC_MODE = not USE_SYNC_MODE
                mode = "åŒæ­¥" if USE_SYNC_MODE else "æµå¼"
                print(f"\nğŸ”„ å·²åˆ‡æ¢åˆ°{mode}æ¨¡å¼")
                continue

            if user_input.lower() == "stream":
                STREAM_MODE = "nodes" if STREAM_MODE == "tokens" else "tokens"
                mode_name = "Tokençº§åˆ«" if STREAM_MODE == "tokens" else "èŠ‚ç‚¹çº§åˆ«"
                print(f"\nğŸ”„ å·²åˆ‡æ¢åˆ°{mode_name}æµå¼æ¨¡å¼")
                continue

            if USE_SYNC_MODE:
                sync_response(user_input, config)
            elif STREAM_MODE == "tokens":
                await stream_tokens_response(user_input, config)
            else:
                await stream_response(user_input, config)

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    import asyncio

    asyncio.run(chat_loop())
