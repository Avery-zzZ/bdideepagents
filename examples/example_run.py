"""
å®Œæ•´ç¤ºä¾‹ï¼šä½¿ç”¨ DeepSeek è¿è¡Œ deepagents
- æ”¯æŒå¤šè½®è¿ç»­å¯¹è¯
- é›†æˆ Tavily æœç´¢
- FilesystemBackend çœŸå®æ–‡ä»¶è¯»å†™
- SubAgent å­ä»£ç†
- LangSmith è¿½è¸ªï¼ˆä¸ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
"""

import os
import uuid
from typing import Literal

from dotenv import load_dotenv
from tavily import TavilyClient
from langchain.chat_models import init_chat_model
from langchain_core.tools import tool
from langgraph.checkpoint.memory import MemorySaver
from deepagents import create_deep_agent
from deepagents.backends import FilesystemBackend

# LangSmith è¿½è¸ªï¼ˆä¸ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
from langsmith import Client as LangSmithClient, tracing_context

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# ============ LangSmith é…ç½® ============
# ç›´æ¥åœ¨ä»£ç ä¸­é…ç½®ï¼Œæ— éœ€è®¾ç½®ç¯å¢ƒå˜é‡
LANGSMITH_API_KEY = os.getenv("LANGSMITH_API_KEY")
LANGSMITH_PROJECT = "deepagents-demo"  # é¡¹ç›®åç§°ï¼Œå¯è‡ªå®šä¹‰
LANGSMITH_ENABLED = False  # é»˜è®¤ç¦ç”¨ï¼Œå›½å†…ç½‘ç»œè®¿é—® LangSmith ä¸ç¨³å®š

# åˆ›å»º LangSmith å®¢æˆ·ç«¯ï¼ˆå¢åŠ è¶…æ—¶è®¾ç½®ï¼‰
langsmith_client = LangSmithClient(
    api_key=LANGSMITH_API_KEY,
    api_url="https://api.smith.langchain.com",
    timeout_ms=10000,  # 10ç§’è¶…æ—¶
)

# åˆå§‹åŒ– Tavily å®¢æˆ·ç«¯
tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))

# å·¥ä½œç›®å½•ï¼ˆç”¨äºæ–‡ä»¶æ“ä½œï¼‰
WORKSPACE_DIR = os.path.join(os.path.dirname(__file__), "temp")
os.makedirs(WORKSPACE_DIR, exist_ok=True)


# ç½‘ç»œæœç´¢å·¥å…·
def internet_search(
    query: str,
    max_results: int = 5,
    topic: Literal["general", "news", "finance"] = "general",
    include_raw_content: bool = False,
):
    """è¿è¡Œç½‘ç»œæœç´¢ã€‚
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        max_results: æœ€å¤§ç»“æœæ•°é‡
        topic: æœç´¢ä¸»é¢˜ç±»å‹ (general/news/finance)
        include_raw_content: æ˜¯å¦åŒ…å«åŸå§‹å†…å®¹
    """
    return tavily_client.search(
        query,
        max_results=max_results,
        include_raw_content=include_raw_content,
        topic=topic,
    )


# ============ è‡ªå®šä¹‰å·¥å…· ============

@tool
def get_current_time() -> str:
    """è·å–å½“å‰æ—¶é—´ã€‚æ¶‰åŠåˆ°æ—¶é—´æœ‰å…³çš„æ“ä½œï¼Œéƒ½éœ€è¦å…ˆæŸ¥è¯¢æ—¶é—´"""
    from datetime import datetime
    now = datetime.now()
    return f"å½“å‰æ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')}"


# ============ å®šä¹‰å­ä»£ç† ============
# ç ”ç©¶å­ä»£ç†ï¼šä¸“é—¨ç”¨äºç½‘ç»œæœç´¢
research_subagent = {
    "name": "researcher",
    "description": "ä¸“é—¨ç”¨äºç½‘ç»œæœç´¢å’Œç ”ç©¶é—®é¢˜çš„å­ä»£ç†ã€‚å½“éœ€è¦æœç´¢äº’è”ç½‘è·å–æœ€æ–°ä¿¡æ¯æ—¶ä½¿ç”¨ã€‚",
    "system_prompt": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. ä½¿ç”¨ internet_search å·¥å…·æœç´¢ç›¸å…³ä¿¡æ¯
2. æ•´ç†å¹¶æ€»ç»“æœç´¢ç»“æœ
3. æä¾›æ¸…æ™°ã€å‡†ç¡®çš„ç ”ç©¶æŠ¥å‘Š

è¯·å§‹ç»ˆä¿æŒå®¢è§‚å’Œå‡†ç¡®ã€‚""",
    "tools": [internet_search],
}


# ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£è°ƒç”¨ DeepSeek
# .env ä¸­å·²é…ç½® OPENAI_API_KEY å’Œ OPENAI_BASE_URL æŒ‡å‘ DeepSeek
model = init_chat_model(
    model="openai:deepseek-chat",  # ä½¿ç”¨ deepseek-chat æ¨¡å‹
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_BASE_URL"),
    max_tokens=8192,  # å¢åŠ è¾“å‡º token é™åˆ¶ï¼ˆDeepSeek æœ€å¤§æ”¯æŒ 8Kï¼‰
)

# ç³»ç»Ÿæç¤º
system_prompt = """ä½ æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ AI åŠ©æ‰‹ï¼Œå…·æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š

## æ ¸å¿ƒèƒ½åŠ›
1. **ä»»åŠ¡è§„åˆ’**: ä½¿ç”¨ write_todos å·¥å…·æ¥è§„åˆ’å’Œè·Ÿè¸ªå¤æ‚ä»»åŠ¡
2. **æ–‡ä»¶ç®¡ç†**: ä½¿ç”¨ ls, read_file, write_file, edit_file å·¥å…·ç®¡ç†çœŸå®æ–‡ä»¶ç³»ç»Ÿ
3. **å­ä»£ç†è°ƒç”¨**: ä½¿ç”¨ task å·¥å…·è°ƒç”¨ä¸“é—¨çš„å­ä»£ç†ï¼š
   - `researcher`: ç”¨äºç½‘ç»œæœç´¢å’Œç ”ç©¶
4. **æ—¶é—´æŸ¥è¯¢**: ä½¿ç”¨ get_current_time è·å–å½“å‰æ—¶é—´

## æ–‡ä»¶ç³»ç»Ÿ
- å·¥ä½œç›®å½•: workspace/
- ä½ å¯ä»¥åœ¨è¿™ä¸ªç›®å½•ä¸‹åˆ›å»ºã€è¯»å–ã€ç¼–è¾‘æ–‡ä»¶
- ä½¿ç”¨ ls æŸ¥çœ‹ç›®å½•å†…å®¹

## å·¥ä½œæµç¨‹
1. æ”¶åˆ°å¤æ‚ä»»åŠ¡æ—¶ï¼Œå…ˆä½¿ç”¨ write_todos åˆ¶å®šè®¡åˆ’
2. æŒ‰è®¡åˆ’é€æ­¥æ‰§è¡Œï¼Œæ›´æ–°ä»»åŠ¡çŠ¶æ€
3. éœ€è¦ç½‘ç»œæœç´¢æ—¶ï¼Œè°ƒç”¨ researcher å­ä»£ç†
4. éœ€è¦ä¿å­˜ç»“æœæ—¶ï¼Œå†™å…¥æ–‡ä»¶ç³»ç»Ÿ
"""

# åˆ›å»º FilesystemBackendï¼ˆçœŸå®æ–‡ä»¶ç³»ç»Ÿï¼‰
filesystem_backend = FilesystemBackend(
    root_dir=WORKSPACE_DIR,
    virtual_mode=True,  # å®‰å…¨æ¨¡å¼ï¼Œé™åˆ¶åœ¨ workspace ç›®å½•å†…
)

# åˆ›å»º checkpointer ç”¨äºä¿å­˜å¯¹è¯çŠ¶æ€
checkpointer = MemorySaver()

# åˆ›å»º deep agentï¼ˆå¸¦ checkpointerã€backendã€subagentsï¼‰
agent = create_deep_agent(
    model=model,
    tools=[get_current_time],  # ä¸»ä»£ç†çš„å·¥å…·
    system_prompt=system_prompt,
    checkpointer=checkpointer,  # å¯ç”¨å¯¹è¯è®°å¿†
    backend=filesystem_backend,  # çœŸå®æ–‡ä»¶ç³»ç»Ÿ
    subagents=[research_subagent],  # å­ä»£ç†
)


# æ˜¯å¦æ˜¾ç¤ºå­æ™ºèƒ½ä½“å†…éƒ¨ç»†èŠ‚
SHOW_SUBAGENT_DETAILS = True

# æ˜¯å¦ä½¿ç”¨åŒæ­¥æ¨¡å¼ï¼ˆinvoke è€Œä¸æ˜¯ astreamï¼‰
USE_SYNC_MODE = False

# æµå¼æ¨¡å¼: "tokens" = tokençº§åˆ«æµå¼, "nodes" = èŠ‚ç‚¹çº§åˆ«æµå¼
STREAM_MODE = "tokens"  # æ”¹ä¸º "nodes" å¯ä»¥åˆ‡æ¢å›èŠ‚ç‚¹çº§åˆ«


# åŒæ­¥æ¨¡å¼å“åº”
def sync_response(user_input: str, config: dict):
    """åŒæ­¥è°ƒç”¨ agentï¼Œç­‰å¾…å®Œæ•´å“åº”"""
    result = agent.invoke(
        {"messages": [{"role": "user", "content": user_input}]},
        config=config,
    )
    
    # æ‰“å°æ‰€æœ‰æ¶ˆæ¯
    for msg in result.get("messages", []):
        msg_type = getattr(msg, "type", type(msg).__name__)
        content = getattr(msg, "content", "")
        
        if msg_type == "human":
            continue  # è·³è¿‡ç”¨æˆ·æ¶ˆæ¯
        
        if msg_type == "ai":
            if content:
                print(f"\nğŸ’¬ AI: {content}")
            
            tool_calls = getattr(msg, "tool_calls", [])
            if tool_calls:
                for tc in tool_calls:
                    print(f"\nğŸ”§ è°ƒç”¨å·¥å…·: {tc.get('name', 'unknown')} | å‚æ•°: {tc.get('args', {})}")
        
        elif msg_type == "tool":
            tool_name = getattr(msg, "name", "unknown")
            display_content = str(content)[:300] + "..." if len(str(content)) > 300 else content
            print(f"\nğŸ“¦ [{tool_name}]: {display_content}")


# Token çº§åˆ«æµå¼è¾“å‡º
async def stream_tokens_response(user_input: str, config: dict):
    """Token çº§åˆ«æµå¼è¾“å‡ºï¼Œæ¯ä¸ª token å®æ—¶æ˜¾ç¤º"""
    print("\nğŸ’¬ AI: ", end="", flush=True)
    
    try:
        async for msg_chunk, metadata in agent.astream(
            {"messages": [{"role": "user", "content": user_input}]},
            config=config,
            stream_mode="messages",  # å…³é”®ï¼šä½¿ç”¨ messages æ¨¡å¼è·å– token çº§åˆ«æµå¼
        ):
            # msg_chunk æ˜¯ AIMessageChunk æˆ–å…¶ä»–æ¶ˆæ¯ç±»å‹
            msg_type = getattr(msg_chunk, "type", type(msg_chunk).__name__)
            
            if msg_type == "AIMessageChunk":
                content = getattr(msg_chunk, "content", "")
                if content:
                    print(content, end="", flush=True)  # å®æ—¶è¾“å‡ºæ¯ä¸ª token
                
                # æ£€æŸ¥å·¥å…·è°ƒç”¨
                tool_calls = getattr(msg_chunk, "tool_calls", [])
                if tool_calls:
                    for tc in tool_calls:
                        if tc.get("name"):
                            print(f"\n\nğŸ”§ è°ƒç”¨å·¥å…·: {tc.get('name')} | å‚æ•°: {tc.get('args', {})}", flush=True)
            
            elif msg_type == "tool":
                tool_name = getattr(msg_chunk, "name", "unknown")
                content = getattr(msg_chunk, "content", "")
                display_content = str(content)[:300] + "..." if len(str(content)) > 300 else content
                print(f"\n\nğŸ“¦ [{tool_name}]: {display_content}", flush=True)
                print("\nğŸ’¬ AI: ", end="", flush=True)  # å‡†å¤‡ä¸‹ä¸€æ®µ AI è¾“å‡º
        
        print()  # æœ€åæ¢è¡Œ
    except Exception as e:
        error_str = str(e).lower()
        if "connection" in error_str or "timeout" in error_str:
            print(f"\n\nâš ï¸ ç½‘ç»œè¿æ¥é—®é¢˜: {e}")
        else:
            print(f"\n\nâŒ Token æµå¼é”™è¯¯: {e}")


# æµå¼è¾“å‡ºå•æ¡æ¶ˆæ¯ï¼ˆå¸¦å­æ™ºèƒ½ä½“ç»†èŠ‚ï¼‰- èŠ‚ç‚¹çº§åˆ«
async def stream_response(user_input: str, config: dict):
    """æµå¼è¾“å‡º agent å“åº”ï¼Œæ”¯æŒæ˜¾ç¤ºå­æ™ºèƒ½ä½“å†…éƒ¨ç»†èŠ‚"""
    
    try:
        if SHOW_SUBAGENT_DETAILS:
            # ä½¿ç”¨ subgraphs=True æ¥æ˜¾ç¤ºå­æ™ºèƒ½ä½“å†…éƒ¨ç»†èŠ‚
            async for chunk in agent.astream(
                {"messages": [{"role": "user", "content": user_input}]},
                config=config,
                stream_mode="updates",  # ä½¿ç”¨ updates æ¨¡å¼è·å–å¢é‡æ›´æ–°
                subgraphs=True,  # å…³é”®å‚æ•°ï¼šæ˜¾ç¤ºå­æ™ºèƒ½ä½“å†…éƒ¨æ‰§è¡Œ
            ):
                # chunk æ˜¯ä¸€ä¸ªå…ƒç»„: (namespace_path, update_dict)
                namespace, update = chunk
                
                # namespace æ˜¯å…ƒç»„ï¼Œè¡¨ç¤ºå½“å‰æ‰§è¡Œè·¯å¾„
                # ä¾‹å¦‚: () è¡¨ç¤ºä¸»æ™ºèƒ½ä½“, ('task:xxx',) è¡¨ç¤ºå­æ™ºèƒ½ä½“
                is_subagent = len(namespace) > 0
                prefix = "    ğŸ”¹ [å­ä»£ç†] " if is_subagent else ""
                
                # è°ƒè¯•ï¼šæ‰“å°æ‰€æœ‰ update çš„é”®ï¼ˆå¯ä»¥æ³¨é‡Šæ‰ï¼‰
                # print(f"  [DEBUG] namespace={namespace}, keys={list(update.keys())}")
                
                # å¤„ç† model èŠ‚ç‚¹çš„è¾“å‡º
                if "model" in update:
                    messages = update["model"].get("messages", [])
                    for msg in messages:
                        msg_type = getattr(msg, "type", type(msg).__name__)
                        content = getattr(msg, "content", "")
                        
                        if msg_type == "ai":
                            if content:
                                print(f"\n{prefix}ğŸ’¬ AI: {content}")
                            
                            tool_calls = getattr(msg, "tool_calls", [])
                            if tool_calls:
                                for tc in tool_calls:
                                    print(f"\n{prefix}ğŸ”§ è°ƒç”¨å·¥å…·: {tc.get('name', 'unknown')} | å‚æ•°: {tc.get('args', {})}")
                
                # å¤„ç† tools èŠ‚ç‚¹çš„è¾“å‡º
                if "tools" in update:
                    messages = update["tools"].get("messages", [])
                    for msg in messages:
                        tool_name = getattr(msg, "name", "unknown")
                        content = getattr(msg, "content", "")
                        display_content = str(content)[:300] + "..." if len(str(content)) > 300 else content
                        print(f"\n{prefix}ğŸ“¦ [{tool_name}]: {display_content}")
                
                # å¤„ç†å…¶ä»–å¯èƒ½çš„èŠ‚ç‚¹ï¼ˆå¦‚ __end__ ç­‰ï¼‰
                # è¿™äº›èŠ‚ç‚¹é€šå¸¸ä¸éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œä½†å¯ä»¥ç”¨äºè°ƒè¯•
        else:
            # ç®€åŒ–æ¨¡å¼ï¼šä¸æ˜¾ç¤ºå­æ™ºèƒ½ä½“å†…éƒ¨ç»†èŠ‚
            async for chunk in agent.astream(
                {"messages": [{"role": "user", "content": user_input}]},
                config=config,
                stream_mode="values"
            ):
                if "messages" in chunk:
                    msg = chunk["messages"][-1]
                    msg_type = getattr(msg, "type", type(msg).__name__)
                    content = getattr(msg, "content", "")
                    
                    if msg_type == "ai":
                        if content:
                            print(f"\nğŸ’¬ AI: {content}")
                        
                        tool_calls = getattr(msg, "tool_calls", [])
                        if tool_calls:
                            for tc in tool_calls:
                                print(f"\nğŸ”§ è°ƒç”¨å·¥å…·: {tc.get('name', 'unknown')} | å‚æ•°: {tc.get('args', {})}")
                    
                    elif msg_type == "tool":
                        tool_name = getattr(msg, "name", "unknown")
                        display_content = str(content)[:300] + "..." if len(str(content)) > 300 else content
                        print(f"\nğŸ“¦ [{tool_name}]: {display_content}")
    except Exception as e:
        error_msg = str(e)
        if "tool_calls" in error_msg and "tool messages" in error_msg:
            print("\nâš ï¸ å¯¹è¯å†å²æŸåï¼Œè¯·è¾“å…¥ 'new' å¼€å§‹æ–°å¯¹è¯")
        else:
            raise


# å¤šè½®å¯¹è¯ä¸»å¾ªç¯
async def chat_loop():
    """äº¤äº’å¼å¤šè½®å¯¹è¯"""
    # å£°æ˜å…¨å±€å˜é‡ï¼ˆå¿…é¡»åœ¨ä½¿ç”¨å‰å£°æ˜ï¼‰
    global SHOW_SUBAGENT_DETAILS, USE_SYNC_MODE, STREAM_MODE, LANGSMITH_ENABLED
    
    # åˆ›å»ºå”¯ä¸€çš„å¯¹è¯çº¿ç¨‹ ID
    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}
    
    print("=" * 60)
    print("ğŸ¤– DeepAgents å¤šè½®å¯¹è¯æ¨¡å¼")
    print("=" * 60)
    print(f"ğŸ“Œ å¯¹è¯çº¿ç¨‹ ID: {thread_id}")
    print(f"ğŸ“ å·¥ä½œç›®å½•: {WORKSPACE_DIR}")
    if LANGSMITH_ENABLED:
        print(f"ğŸ“Š LangSmith è¿½è¸ª: å·²å¯ç”¨ (é¡¹ç›®: {LANGSMITH_PROJECT})")
    else:
        print("ğŸ“Š LangSmith è¿½è¸ª: å·²ç¦ç”¨")
    print("ğŸ’¡ è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºå¯¹è¯")
    print("ğŸ’¡ è¾“å…¥ 'new' å¼€å§‹æ–°å¯¹è¯")
    print("ğŸ’¡ è¾“å…¥ 'toggle' åˆ‡æ¢å­ä»£ç†ç»†èŠ‚æ˜¾ç¤º")
    print("ğŸ’¡ è¾“å…¥ 'sync' åˆ‡æ¢åŒæ­¥/æµå¼æ¨¡å¼")
    print("ğŸ’¡ è¾“å…¥ 'stream' åˆ‡æ¢ token/èŠ‚ç‚¹ çº§åˆ«æµå¼")
    print("ğŸ’¡ è¾“å…¥ 'trace' åˆ‡æ¢ LangSmith è¿½è¸ªå¼€å…³")
    print("=" * 60)

    while True:
        try:
            user_input = input("\nğŸ‘¤ ä½ : ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ["quit", "exit", "q"]:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            if user_input.lower() == "new":
                thread_id = str(uuid.uuid4())
                config = {"configurable": {"thread_id": thread_id}}
                print(f"\nğŸ”„ å·²å¼€å§‹æ–°å¯¹è¯ï¼Œçº¿ç¨‹ ID: {thread_id}")
                continue
            
            if user_input.lower() == "toggle":
                SHOW_SUBAGENT_DETAILS = not SHOW_SUBAGENT_DETAILS
                status = "å¼€å¯" if SHOW_SUBAGENT_DETAILS else "å…³é—­"
                print(f"\nğŸ”„ å­ä»£ç†ç»†èŠ‚æ˜¾ç¤ºå·²{status}")
                continue
            
            if user_input.lower() == "sync":
                USE_SYNC_MODE = not USE_SYNC_MODE
                mode = "åŒæ­¥" if USE_SYNC_MODE else "æµå¼"
                print(f"\nğŸ”„ å·²åˆ‡æ¢åˆ°{mode}æ¨¡å¼")
                continue
            
            if user_input.lower() == "stream":
                STREAM_MODE = "nodes" if STREAM_MODE == "tokens" else "tokens"
                mode_name = "Tokençº§åˆ«" if STREAM_MODE == "tokens" else "èŠ‚ç‚¹çº§åˆ«"
                print(f"\nğŸ”„ å·²åˆ‡æ¢åˆ°{mode_name}æµå¼æ¨¡å¼")
                continue
            
            if user_input.lower() == "trace":
                LANGSMITH_ENABLED = not LANGSMITH_ENABLED
                status = "å¼€å¯" if LANGSMITH_ENABLED else "å…³é—­"
                print(f"\nğŸ”„ LangSmith è¿½è¸ªå·²{status}")
                continue
            
            # ä½¿ç”¨ tracing_context åŒ…è£¹è°ƒç”¨ï¼ˆä¸ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
            try:
                with tracing_context(
                    client=langsmith_client,
                    project_name=LANGSMITH_PROJECT,
                    enabled=LANGSMITH_ENABLED,
                ):
                    # æ ¹æ®æ¨¡å¼é€‰æ‹©è°ƒç”¨æ–¹å¼
                    if USE_SYNC_MODE:
                        sync_response(user_input, config)
                    elif STREAM_MODE == "tokens":
                        await stream_tokens_response(user_input, config)
                    else:
                        await stream_response(user_input, config)
            except Exception as trace_error:
                # å¦‚æœæ˜¯ LangSmith è¿½è¸ªç›¸å…³çš„è¿æ¥é”™è¯¯ï¼Œæç¤ºä½†ç»§ç»­è¿è¡Œ
                error_str = str(trace_error).lower()
                if "connection" in error_str or "timeout" in error_str:
                    print("\nâš ï¸ LangSmith è¿½è¸ªä¸Šä¼ å¤±è´¥ï¼ˆç½‘ç»œé—®é¢˜ï¼‰ï¼Œä½†å¯¹è¯å·²å®Œæˆ")
                    print("   æç¤ºï¼šè¾“å…¥ 'trace' å¯ç¦ç”¨è¿½è¸ª")
                else:
                    raise  # å…¶ä»–é”™è¯¯ç»§ç»­æŠ›å‡º
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    import asyncio
    asyncio.run(chat_loop())
